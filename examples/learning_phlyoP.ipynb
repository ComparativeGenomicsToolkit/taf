{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PhyloP Scores\n",
    "\n",
    "This is just vanilla pytorch to train a network for each of the different alignments and report the loss and accuracy\n",
    "Here we convert the phyloP scores into either \"conserved\" / \"non-conserved\" and learn to predict these classifications.\n",
    "\n",
    "The alignment file example is very small so accuracy is not so good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load necessary dependencies\n",
    "import os \n",
    "import time\n",
    "import taffy\n",
    "import taffy.lib\n",
    "import taffy.ml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "## Decide what kind of architecture we're running on\n",
    "device = (  \n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+../tests/447-way/example_norm.sh:16> alignment_file_prefix=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000 \n",
      "+../tests/447-way/example_norm.sh:20> no_masking_alignment_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_with_ancestors.taf.gz \n",
      "+../tests/447-way/example_norm.sh:21> echo 'No masking of bases file ' /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_with_ancestors.taf.gz\n",
      "No masking of bases file  /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_with_ancestors.taf.gz\n",
      "+../tests/447-way/example_norm.sh:23> reference_masking_with_ancestors_alignment_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.reference_masking_with_ancestors.taf.gz \n",
      "+../tests/447-way/example_norm.sh:24> echo 'Reference masking with ancestors file ' /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.reference_masking_with_ancestors.taf.gz\n",
      "Reference masking with ancestors file  /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.reference_masking_with_ancestors.taf.gz\n",
      "+../tests/447-way/example_norm.sh:26> lineage_masking_alignment_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.lineage_masking.taf.gz \n",
      "+../tests/447-way/example_norm.sh:27> echo 'Lineage masking file ,' /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.lineage_masking.taf.gz\n",
      "Lineage masking file , /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.lineage_masking.taf.gz\n",
      "+../tests/447-way/example_norm.sh:29> no_masking_no_ancestors_alignment_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_no_ancestors.taf.gz \n",
      "+../tests/447-way/example_norm.sh:30> echo 'No masking, no ancestors file ' /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_no_ancestors.taf.gz\n",
      "No masking, no ancestors file  /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_no_ancestors.taf.gz\n",
      "+../tests/447-way/example_norm.sh:32> reference_masking_no_ancestors_alignment_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.reference_masking_no_ancestors.taf.gz \n",
      "+../tests/447-way/example_norm.sh:33> echo 'Reference masking, no ancestors file,' /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.reference_masking_no_ancestors.taf.gz\n",
      "Reference masking, no ancestors file, /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.reference_masking_no_ancestors.taf.gz\n",
      "+../tests/447-way/example_norm.sh:36> tree_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.nh \n",
      "+../tests/447-way/example_norm.sh:39> rerooted_tree_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.rerooted.nh \n",
      "+../tests/447-way/example_norm.sh:42> wig_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.phyloP.wig \n",
      "+../tests/447-way/example_norm.sh:45> alignment_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.anc.norm.taf.gz \n",
      "+../tests/447-way/example_norm.sh:48> sort_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt \n",
      "+../tests/447-way/example_norm.sh:51> filter_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-filter.txt \n",
      "+../tests/447-way/example_norm.sh:54> ref=hg38 \n",
      "+../tests/447-way/example_norm.sh:57> /Users/benedictpaten/CLionProjects/taffy/examples/..//scripts/manipulate_tree.py --reroot hg38 --out_file /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.rerooted.nh /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.nh\n",
      "${taffy_root}/scripts/manipulate_tree.py --reroot $ref --out_file  $tree_file  0.02s user 0.01s system 91% cpu 0.033 total\n",
      "+../tests/447-way/example_norm.sh:60> /Users/benedictpaten/CLionProjects/taffy/examples/..//scripts/tree_to_sort_file.py --traversal pre --reroot hg38 --out_file /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt --suffix_to_append_to_labels . /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.nh\n",
      "wooooo .\n",
      "${taffy_root}/scripts/tree_to_sort_file.py --traversal pre --reroot $ref    .  0.05s user 0.01s system 93% cpu 0.062 total\n",
      "+../tests/447-way/example_norm.sh:63> /Users/benedictpaten/CLionProjects/taffy/examples/..//scripts/tree_to_sort_file.py --out_file /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-filter.txt --no_leaf_nodes --suffix_to_append_to_labels . /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.nh\n",
      "wooooo .\n",
      "${taffy_root}/scripts/tree_to_sort_file.py --out_file $filter_file   .   0.03s user 0.01s system 93% cpu 0.034 total\n",
      "+../tests/447-way/example_norm.sh:68> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy sort -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.anc.norm.taf.gz -n /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt -p /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt -d /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt\n",
      "+../tests/447-way/example_norm.sh:68> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy annotate -w /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.phyloP.wig --tagName phyloP --refPrefix hg38. -c\n",
      "+../tests/447-way/example_norm.sh:68> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy view -c --runLengthEncodeBases\n",
      "${taffy_root}/bin/taffy sort -i ${alignment_file} -n ${sort_file} -p  -d   70.40s user 2.41s system 93% cpu 1:18.04 total\n",
      "${taffy_root}/bin/taffy annotate -w ${wig_file} --tagName phyloP --refPrefix   16.39s user 0.10s system 21% cpu 1:18.06 total\n",
      "${taffy_root}/bin/taffy view -c --runLengthEncodeBases >   5.18s user 0.05s system 6% cpu 1:18.08 total\n",
      "+../tests/447-way/example_norm.sh:69> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy index -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_with_ancestors.taf.gz\n",
      "${taffy_root}/bin/taffy index -i ${no_masking_alignment_file}  0.61s user 0.00s system 99% cpu 0.620 total\n",
      "+../tests/447-way/example_norm.sh:74> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy view -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_with_ancestors.taf.gz -a -c\n",
      "${taffy_root}/bin/taffy view -i ${no_masking_alignment_file} -a -c >   16.35s user 0.04s system 99% cpu 16.435 total\n",
      "+../tests/447-way/example_norm.sh:75> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy index -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.reference_masking_with_ancestors.taf.gz\n",
      "${taffy_root}/bin/taffy index -i   0.46s user 0.01s system 99% cpu 0.464 total\n",
      "+../tests/447-way/example_norm.sh:78> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy view -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_with_ancestors.taf.gz -t /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.rerooted.nh -b -c\n",
      "${taffy_root}/bin/taffy view -i ${no_masking_alignment_file} -t  -b -c >   17.39s user 0.08s system 99% cpu 17.511 total\n",
      "+../tests/447-way/example_norm.sh:79> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy index -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.lineage_masking.taf.gz\n",
      "${taffy_root}/bin/taffy index -i ${lineage_masking_alignment_file}  0.46s user 0.01s system 99% cpu 0.463 total\n",
      "+../tests/447-way/example_norm.sh:82> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy sort -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_with_ancestors.taf.gz -f /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-filter.txt -c\n",
      "${taffy_root}/bin/taffy sort -i ${no_masking_alignment_file} -f ${filter_file  3.72s user 0.02s system 99% cpu 3.754 total\n",
      "+../tests/447-way/example_norm.sh:83> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy index -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_no_ancestors.taf.gz\n",
      "${taffy_root}/bin/taffy index -i ${no_masking_no_ancestors_alignment_file}  0.38s user 0.00s system 99% cpu 0.389 total\n",
      "+../tests/447-way/example_norm.sh:86> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy sort -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_with_ancestors.taf.gz -f /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-filter.txt -c\n",
      "+../tests/447-way/example_norm.sh:86> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy view -a -c\n",
      "${taffy_root}/bin/taffy sort -i ${no_masking_alignment_file} -f ${filter_file  3.83s user 0.02s system 47% cpu 8.106 total\n",
      "${taffy_root}/bin/taffy view -a -c >   8.17s user 0.02s system 99% cpu 8.212 total\n",
      "+../tests/447-way/example_norm.sh:87> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy index -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.reference_masking_no_ancestors.taf.gz\n",
      "${taffy_root}/bin/taffy index -i   0.27s user 0.00s system 99% cpu 0.277 total\n",
      "+../tests/447-way/example_norm.sh:91> echo 'Starting alignment'\n",
      "Starting alignment\n",
      "+../tests/447-way/example_norm.sh:92> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy stats -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.anc.norm.taf.gz -a\n",
      "Total blocks:\t3166\n",
      "Total columns:\t215664\n",
      "Avg. columns/block:\t68.118759\n",
      "Total bases:\t67026589\n",
      "Total gaps:\t226206389\n",
      "Avg. column depth:\t1359.675171\n",
      "Avg. bases/column:\t310.791718\n",
      "Avg. gaps/column:\t1048.883423\n",
      "${taffy_root}/bin/taffy stats -i ${alignment_file} -a  9.04s user 0.06s system 99% cpu 9.123 total\n",
      "+../tests/447-way/example_norm.sh:94> echo 'Stats of final alignments'\n",
      "Stats of final alignments\n",
      "+../tests/447-way/example_norm.sh:95> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy stats -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.no_masking_with_ancestors.taf.gz -a\n",
      "Total blocks:\t3166\n",
      "Total columns:\t215664\n",
      "Avg. columns/block:\t68.118759\n",
      "Total bases:\t25013474\n",
      "Total gaps:\t167358814\n",
      "Avg. column depth:\t892.000000\n",
      "Avg. bases/column:\t115.983536\n",
      "Avg. gaps/column:\t776.016479\n",
      "${taffy_root}/bin/taffy stats -i ${no_masking_alignment_file} -a  1.80s user 0.02s system 99% cpu 1.830 total\n"
     ]
    }
   ],
   "source": [
    "## Run shell script to generate normalized alignment files and annotate with wig (comment this out if already run)\n",
    "!../tests/447-way/example_norm.sh `pwd`/../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Annotated alignment files\n",
    "\n",
    "alignment_file_prefix = os.path.join(os.getcwd(), \"../tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000\")\n",
    "\n",
    "alignment_files = { # (1) No masking bases\n",
    "                    \"no masking, with ancestors\": alignment_file_prefix + \".no_masking_with_ancestors.taf.gz\", \n",
    "                    # (2) Reference masking of bases\n",
    "                    \"reference masking, with ancestors\": alignment_file_prefix + \".reference_masking_with_ancestors.taf.gz\", \n",
    "                    # (3) Lineage masking of bases\n",
    "                    \"lineage masking\": alignment_file_prefix + \".lineage_masking.taf.gz\",\n",
    "                    # (4) No masking bases, no ancestors\n",
    "                    \"no masking, no ancestors\": alignment_file_prefix + \".no_masking_no_ancestors.taf.gz\",\n",
    "                    # (5) Reference masking of bases, no ancestors\n",
    "                    \"reference masking, no ancestors\": alignment_file_prefix + \".reference_masking_no_ancestors.taf.gz\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 4.771432161331177 seconds to scan the no masking, with ancestors\n",
      "It took 4.5472259521484375 seconds to scan the reference masking, with ancestors\n",
      "It took 4.355786085128784 seconds to scan the lineage masking\n",
      "It took 2.2986509799957275 seconds to scan the no masking, no ancestors\n",
      "It took 1.986997127532959 seconds to scan the reference masking, no ancestors\n"
     ]
    }
   ],
   "source": [
    "## If you're curious, here is the time it takes to do a scan of the columns and tags in the alignment files\n",
    "# (no need to run this otherwise)\n",
    "import time\n",
    "\n",
    "for alignment_name in alignment_files:\n",
    "    alignment_file = alignment_files[alignment_name]\n",
    "    taf_index = taffy.lib.TafIndex(alignment_file + \".tai\", is_maf=False)\n",
    "\n",
    "    ## Get the names of the sequences in the alignment (this involves a scan of the underlying file)\n",
    "    \n",
    "    # First make an alignment reader\n",
    "    with taffy.lib.AlignmentReader(alignment_file) as ar:\n",
    "        # Now get the intervals\n",
    "        sequence_intervals = list(taffy.lib.get_reference_sequence_intervals(ar))\n",
    "    \n",
    "    start = time.time()\n",
    "    with taffy.lib.AlignmentReader(alignment_file, taf_index=taf_index, sequence_intervals=sequence_intervals) as ar:\n",
    "        for column in taffy.lib.get_column_iterator(ar,\n",
    "                            include_sequence_names=False,\n",
    "                            include_non_ref_columns=False,\n",
    "                            include_column_tags=True):\n",
    "            pass\n",
    "    print(f\"It took {time.time()-start} seconds to scan the {alignment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of no masking, with ancestors is 892\n",
      "Depth of reference masking, with ancestors is 892\n",
      "Depth of lineage masking is 892\n",
      "Depth of no masking, no ancestors is 447\n",
      "Depth of reference masking, no ancestors is 447\n"
     ]
    }
   ],
   "source": [
    "## Here is code to create a pytorch DataLoader object from the alignments\n",
    "\n",
    "## Create a DataLoader for the alignment, print the first few entries and establish the alignment depth\n",
    "def get_alignment_iterator(alignment_file, batch_size=10, num_workers=1, window_length=1, number_of_partitions=1, partition_index=0):\n",
    "    # The index file\n",
    "    index_file = alignment_file + \".tai\"\n",
    "    \n",
    "    ## Get the names of the sequences in the alignment (this involves a scan of the underlying file)\n",
    "    # First make an alignment reader\n",
    "    with taffy.lib.AlignmentReader(alignment_file) as ar:\n",
    "        # Now get the intervals\n",
    "        sequence_intervals = list(taffy.lib.get_reference_sequence_intervals(ar))\n",
    "        # Now partition to them to what we want\n",
    "        sequence_intervals = taffy.ml.get_subsequence_intervals(sequence_intervals=sequence_intervals, \n",
    "                                                                number_of_partitions=number_of_partitions,\n",
    "                                                                partition_index=partition_index)\n",
    "    \n",
    "    ## Now make the iterator\n",
    "    alignment_iterator = DataLoader(taffy.ml.TorchDatasetAlignmentIterator(alignment_file, \n",
    "                                                                            label_conversion_function=taffy.ml.get_phyloP_labels \\\n",
    "                                                                            if window_length > 1 else taffy.ml.get_phyloP_label,\n",
    "                                                                            taf_index_file=index_file, \n",
    "                                                                            is_maf=False,\n",
    "                                                                            sequence_intervals=sequence_intervals, \n",
    "                                                                            window_length=window_length,\n",
    "                                                                            step=1,\n",
    "                                                                            include_non_ref_columns=False,\n",
    "                                                                            include_sequence_names=False, \n",
    "                                                                            include_column_tags=True,\n",
    "                                                                            column_one_hot=True),\n",
    "                                    batch_size=batch_size, num_workers=num_workers)\n",
    "    return alignment_iterator\n",
    "\n",
    "## Function to get the alignment depth\n",
    "def get_alignment_depth(alignment_file):\n",
    "    for (column, labels) in get_alignment_iterator(alignment_file):\n",
    "        return len(column[0])\n",
    "\n",
    "for alignment_name in alignment_files:\n",
    "    alignment_file = alignment_files[alignment_name]\n",
    "    print(f\"Depth of {alignment_name} is {get_alignment_depth(alignment_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alignment no masking, with ancestors with depth: 892 and window length: 1, got model NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5352, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Make a suitably tiny neural network\n",
    "\n",
    "def get_model(alignment_depth, window_length):\n",
    "    \"\"\" Make a v. basic NN model suitable for learning phyloP params\n",
    "    \"\"\"\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(window_length * alignment_depth * 6, 512),  # The 6 is because we have a 6 base encoding of entries in the column\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 2),\n",
    "            )\n",
    "            #self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "        def forward(self, X):\n",
    "            x = self.flatten(X)\n",
    "            logits = self.linear_relu_stack(x)\n",
    "            return logits\n",
    "            #return self.softmax(logits)\n",
    "    \n",
    "    model = NeuralNetwork().to(device)\n",
    "    return model\n",
    "\n",
    "# Quick demo code\n",
    "alignment_name = list(alignment_files.keys())[0]\n",
    "alignment_file = alignment_files[alignment_name]\n",
    "alignment_depth = get_alignment_depth(alignment_file)  # Get the depth of the alignment\n",
    "window_length = 1\n",
    "model = get_model(alignment_depth, window_length)\n",
    "print(f\"For alignment {alignment_name} with depth: {alignment_depth} and window length: {window_length}, got model {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5352, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Training no masking, with ancestors\n",
      "Training Error -- Epoch: 0, Accuracy: 75.7%, Avg loss: 0.485613,             in 19.398117780685425 seconds\n",
      "Training Error -- Epoch: 5, Accuracy: 86.8%, Avg loss: 0.308761,             in 98.34846115112305 seconds\n"
     ]
    }
   ],
   "source": [
    "## Training functions\n",
    "\n",
    "def to_class(Y, window_length):\n",
    "    \"\"\" Convert Y to a one-hot classification vector of the middle base\n",
    "    \"\"\"\n",
    "    Y_class = torch.zeros((len(Y), 2)) \n",
    "    for i, window in enumerate(Y):\n",
    "        v = window if window_length == 1 else window[len(window)//2]  # This deals with the case that\n",
    "        # when window_length = 1 the iterator just returns a single value\n",
    "        if v > 0:\n",
    "            Y_class[i,0] = 1 \n",
    "        else:\n",
    "            Y_class[i,1] = 1 \n",
    "    return Y_class\n",
    "\n",
    "def train(alignment_file, \n",
    "          window_length, \n",
    "          model, \n",
    "          loss_fn, \n",
    "          optimizer, \n",
    "          number_of_partitions,\n",
    "          partition_index,\n",
    "          epoch_number, \n",
    "          batch_size,\n",
    "          num_workers):\n",
    "    \"\"\" Creates a model, trains it\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_number):\n",
    "        dataloader = get_alignment_iterator(alignment_file=alignment_file, \n",
    "                                            batch_size=batch_size, \n",
    "                                            num_workers=num_workers, \n",
    "                                            window_length=window_length,\n",
    "                                            number_of_partitions=number_of_partitions, \n",
    "                                            partition_index=partition_index)\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total_batches = 0\n",
    "        total_examples = 0\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            y = to_class(y, window_length)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Track correctness on training data\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "            total_examples += len(y)\n",
    "\n",
    "        if epoch % 5 == 0:  # Report accuracy every 5 epochs\n",
    "            total_loss /= total_batches\n",
    "            correct /= total_examples\n",
    "            print(f\"Training Error -- Epoch: {epoch}, Accuracy: {(100*correct):>0.1f}%, Avg loss: {total_loss:>8f}, \\\n",
    "            in {time.time()-start_time} seconds\")\n",
    "            start_time = time.time()\n",
    "\n",
    "## Code to demo running this training\n",
    "epoch_number = 6\n",
    "batch_size = 64\n",
    "window_length = 1\n",
    "num_workers = 1\n",
    "alignment_name = list(alignment_files.keys())[0]\n",
    "alignment_file = alignment_files[alignment_name]\n",
    "model = get_model(alignment_depth, window_length)\n",
    "print(model)\n",
    "print(f\"Training {alignment_name}\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train(alignment_file=alignment_file,\n",
    "      window_length=window_length,\n",
    "      model=model,\n",
    "      loss_fn=loss_fn,\n",
    "      optimizer=optimizer,\n",
    "      number_of_partitions=1,\n",
    "      partition_index=0,\n",
    "      epoch_number=epoch_number, \n",
    "      batch_size=batch_size,\n",
    "      num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5352, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Training no masking, with ancestors\n",
      "Training Error -- Epoch: 0, Accuracy: 71.3%, Avg loss: 0.548996,             in 14.536561012268066 seconds\n",
      "Training Error -- Epoch: 5, Accuracy: 84.4%, Avg loss: 0.358975,             in 73.47340297698975 seconds\n",
      "Training Error -- Epoch: 10, Accuracy: 88.3%, Avg loss: 0.281573,             in 74.08111500740051 seconds\n",
      "Training Error -- Epoch: 15, Accuracy: 89.4%, Avg loss: 0.263113,             in 75.13336491584778 seconds\n",
      "Testing no masking, with ancestors\n",
      "Test Error Accuracy: 85.2%, Avg loss: 0.392149,             in 13.888468027114868 seconds\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5352, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Training reference masking, with ancestors\n",
      "Training Error -- Epoch: 0, Accuracy: 70.5%, Avg loss: 0.571036,             in 14.638205766677856 seconds\n",
      "Training Error -- Epoch: 5, Accuracy: 80.8%, Avg loss: 0.416488,             in 71.95761775970459 seconds\n",
      "Training Error -- Epoch: 10, Accuracy: 86.1%, Avg loss: 0.324584,             in 71.84567999839783 seconds\n",
      "Training Error -- Epoch: 15, Accuracy: 89.0%, Avg loss: 0.266199,             in 73.04667496681213 seconds\n",
      "Testing reference masking, with ancestors\n",
      "Test Error Accuracy: 84.7%, Avg loss: 0.418562,             in 12.709128856658936 seconds\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5352, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Training lineage masking\n",
      "Training Error -- Epoch: 0, Accuracy: 69.9%, Avg loss: 0.567547,             in 14.834358215332031 seconds\n",
      "Training Error -- Epoch: 5, Accuracy: 76.5%, Avg loss: 0.473172,             in 73.73221397399902 seconds\n",
      "Training Error -- Epoch: 10, Accuracy: 79.9%, Avg loss: 0.424230,             in 73.83550906181335 seconds\n",
      "Training Error -- Epoch: 15, Accuracy: 82.4%, Avg loss: 0.383180,             in 70.54877805709839 seconds\n",
      "Testing lineage masking\n",
      "Test Error Accuracy: 77.5%, Avg loss: 0.474542,             in 12.36080288887024 seconds\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=2682, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Training no masking, no ancestors\n",
      "Training Error -- Epoch: 0, Accuracy: 70.4%, Avg loss: 0.562716,             in 11.322806119918823 seconds\n",
      "Training Error -- Epoch: 5, Accuracy: 84.4%, Avg loss: 0.357423,             in 57.88431787490845 seconds\n",
      "Training Error -- Epoch: 10, Accuracy: 88.2%, Avg loss: 0.281943,             in 56.69631099700928 seconds\n",
      "Training Error -- Epoch: 15, Accuracy: 90.3%, Avg loss: 0.242530,             in 59.46003985404968 seconds\n",
      "Testing no masking, no ancestors\n",
      "Test Error Accuracy: 86.4%, Avg loss: 0.380739,             in 9.998534917831421 seconds\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=2682, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Training reference masking, no ancestors\n",
      "Training Error -- Epoch: 0, Accuracy: 70.7%, Avg loss: 0.569358,             in 11.104099035263062 seconds\n",
      "Training Error -- Epoch: 5, Accuracy: 80.8%, Avg loss: 0.416258,             in 56.99876523017883 seconds\n",
      "Training Error -- Epoch: 10, Accuracy: 85.9%, Avg loss: 0.329629,             in 56.08124089241028 seconds\n",
      "Training Error -- Epoch: 15, Accuracy: 88.9%, Avg loss: 0.268522,             in 55.96105217933655 seconds\n",
      "Testing reference masking, no ancestors\n",
      "Test Error Accuracy: 85.5%, Avg loss: 0.426293,             in 9.957872867584229 seconds\n"
     ]
    }
   ],
   "source": [
    "## Testing functions\n",
    "def test(alignment_file, model, loss_fn, \n",
    "         window_length,\n",
    "         number_of_partitions,\n",
    "         partition_index,\n",
    "         batch_size,\n",
    "         num_workers):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total_batches = 0\n",
    "    total_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in get_alignment_iterator(alignment_file=alignment_file, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           window_length=window_length,\n",
    "                                           number_of_partitions=number_of_partitions, \n",
    "                                           partition_index=partition_index):\n",
    "            # Predict\n",
    "            y = to_class(y, window_length)\n",
    "            #y = y.float()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Update stats\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "            total_examples += len(y)\n",
    "\n",
    "    total_loss /= total_batches\n",
    "    correct /= total_examples\n",
    "    print(f\"Test Error Accuracy: {(100*correct):>0.1f}%, Avg loss: {total_loss:>8f}, \\\n",
    "            in {time.time()-start_time} seconds\")\n",
    "\n",
    "\n",
    "## Code to train/test each of the different alignments\n",
    "epoch_number = 16 \n",
    "batch_size = 64\n",
    "window_length = 1\n",
    "num_workers = 1\n",
    "for alignment_name in alignment_files:\n",
    "    alignment_file = alignment_files[alignment_name]\n",
    "    alignment_depth = get_alignment_depth(alignment_file)\n",
    "    model = get_model(alignment_depth, window_length)\n",
    "    print(model) # Print the model\n",
    "    print(f\"Training {alignment_name}\")\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    #loss_fn = nn.MSELoss()  # mean square error # nn.CrossEntropyLoss()\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Split the dataset into two, train on the first half, test on the second half\n",
    "    train(alignment_file=alignment_file,\n",
    "          window_length=window_length,\n",
    "          model=model,\n",
    "          loss_fn=loss_fn,\n",
    "          optimizer=optimizer,\n",
    "          number_of_partitions=2,\n",
    "          partition_index=0,\n",
    "          epoch_number=epoch_number, \n",
    "          batch_size=batch_size,\n",
    "          num_workers=num_workers)\n",
    "    \n",
    "    print(f\"Testing {alignment_name}\")\n",
    "    test(alignment_file=alignment_file,\n",
    "         model=model,\n",
    "         loss_fn=loss_fn,\n",
    "         window_length=window_length,\n",
    "         number_of_partitions=2,\n",
    "         partition_index=1,\n",
    "         batch_size=batch_size,\n",
    "         num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
