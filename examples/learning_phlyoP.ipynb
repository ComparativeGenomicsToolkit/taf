{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load necessary dependencies\n",
    "import os\n",
    "import taffy\n",
    "import taffy.lib\n",
    "import taffy.ml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "## Decide what kind of architecture we're running on\n",
    "device = (  \n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+../tests/447-way/example_norm.sh:13> tree_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.nh \n",
      "+../tests/447-way/example_norm.sh:16> rerooted_tree_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.rerooted.nh \n",
      "+../tests/447-way/example_norm.sh:19> wig_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.phyloP.wig \n",
      "+../tests/447-way/example_norm.sh:22> alignment_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.anc.norm.taf.gz \n",
      "+../tests/447-way/example_norm.sh:25> rearranged_alignment_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.rearranged.taf.gz \n",
      "+../tests/447-way/example_norm.sh:28> final_alignment_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.final.taf.gz \n",
      "+../tests/447-way/example_norm.sh:31> sort_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt \n",
      "+../tests/447-way/example_norm.sh:34> filter_file=/Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-filter.txt \n",
      "+../tests/447-way/example_norm.sh:37> ref=hg38 \n",
      "+../tests/447-way/example_norm.sh:40> /Users/benedictpaten/CLionProjects/taffy/examples/..//scripts/manipulate_tree.py --reroot hg38 --out_file /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.rerooted.nh /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.nh\n",
      "${taffy_root}/scripts/manipulate_tree.py --reroot $ref --out_file  $tree_file  0.02s user 0.01s system 88% cpu 0.033 total\n",
      "+../tests/447-way/example_norm.sh:43> /Users/benedictpaten/CLionProjects/taffy/examples/..//scripts/tree_to_sort_file.py --traversal pre --reroot hg38 --out_file /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt --suffix_to_append_to_labels . /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.nh\n",
      "wooooo .\n",
      "${taffy_root}/scripts/tree_to_sort_file.py --traversal pre --reroot $ref    .  0.05s user 0.01s system 94% cpu 0.062 total\n",
      "+../tests/447-way/example_norm.sh:46> /Users/benedictpaten/CLionProjects/taffy/examples/..//scripts/tree_to_sort_file.py --out_file /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-filter.txt --no_leaf_nodes --suffix_to_append_to_labels . /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.nh\n",
      "wooooo .\n",
      "${taffy_root}/scripts/tree_to_sort_file.py --out_file $filter_file   .   0.03s user 0.01s system 93% cpu 0.034 total\n",
      "+../tests/447-way/example_norm.sh:49> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy sort -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.anc.norm.taf.gz -n /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt -p /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt -d /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt --logLevel DEBUG\n",
      "+../tests/447-way/example_norm.sh:49> ../bin/taffy view -t /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1.rerooted.nh -b -c --runLengthEncodeBases\n",
      "Input file string : /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.anc.norm.taf.gz\n",
      "Output file string : (null)\n",
      "Sort file string : /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt\n",
      "Filter file string : (null)\n",
      "Pad file string : /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt\n",
      "Dup filter file string : /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-sort.txt\n",
      "Ignore first row : True\n",
      "Loaded the sort/filter file, got 892 rows\n",
      "Loaded the sort/filter file, got 892 rows\n",
      "Loaded the sort/filter file, got 892 rows\n",
      "taffy sort is done, 75 seconds have elapsed\n",
      "${taffy_root}/bin/taffy sort -i $alignment_file -n $sort_file -p $sort_file -  71.20s user 2.46s system 98% cpu 1:14.66 total\n",
      "../bin/taffy view -t $rerooted_tree_file -b -c --runLengthEncodeBases >   6.22s user 0.14s system 8% cpu 1:14.67 total\n",
      "+../tests/447-way/example_norm.sh:52> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy annotate -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.rearranged.taf.gz -w /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.phyloP.wig --tagName phyloP --refPrefix hg38. -c\n",
      "${taffy_root}/bin/taffy annotate -i $rearranged_alignment_file -w $wig_file    5.28s user 0.03s system 99% cpu 5.325 total\n",
      "+../tests/447-way/example_norm.sh:55> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy index -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.final.taf.gz\n",
      "${taffy_root}/bin/taffy index -i $final_alignment_file  0.62s user 0.01s system 99% cpu 0.623 total\n",
      "+../tests/447-way/example_norm.sh:59> echo 'Starting alignment'\n",
      "Starting alignment\n",
      "+../tests/447-way/example_norm.sh:60> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy stats -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_hg38_chr22_22000000_22100000.anc.norm.taf.gz -a\n",
      "Total blocks:\t3166\n",
      "Total columns:\t215664\n",
      "Avg. columns/block:\t68.118759\n",
      "Total bases:\t67026589\n",
      "Total gaps:\t226206389\n",
      "Avg. column depth:\t1359.675171\n",
      "Avg. bases/column:\t310.791718\n",
      "Avg. gaps/column:\t1048.883423\n",
      "${taffy_root}/bin/taffy stats -i $alignment_file -a  9.08s user 0.08s system 99% cpu 9.207 total\n",
      "+../tests/447-way/example_norm.sh:62> echo 'Final alignment'\n",
      "Final alignment\n",
      "+../tests/447-way/example_norm.sh:63> /Users/benedictpaten/CLionProjects/taffy/examples/..//bin/taffy stats -i /Users/benedictpaten/CLionProjects/taffy/examples/..//tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.final.taf.gz -a\n",
      "Total blocks:\t3166\n",
      "Total columns:\t215664\n",
      "Avg. columns/block:\t68.118759\n",
      "Total bases:\t25013474\n",
      "Total gaps:\t167358814\n",
      "Avg. column depth:\t892.000000\n",
      "Avg. bases/column:\t115.983536\n",
      "Avg. gaps/column:\t776.016479\n",
      "${taffy_root}/bin/taffy stats -i $final_alignment_file -a  1.91s user 0.05s system 95% cpu 2.046 total\n"
     ]
    }
   ],
   "source": [
    "## Run shell script to normalize alignment and annotate with wig (comment this out if already run)\n",
    "!../tests/447-way/example_norm.sh `pwd`/../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alignment file to use: /Users/benedictpaten/CLionProjects/taffy/examples/../tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.final.taf.gz\n"
     ]
    }
   ],
   "source": [
    "## Annotated alignment file\n",
    "#alignment_file = os.path.join(os.getcwd(), \"../tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.rearranged.taf.gz\")\n",
    "alignment_file = os.path.join(os.getcwd(), \"../tests/447-way/447-mammalian-2022v1_chr22_22000000_22100000.final.taf.gz\")\n",
    "print(f\"The alignment file to use: {alignment_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a taf index\n",
    "\n",
    "# Write the index file\n",
    "index_file = alignment_file + \".tai\"\n",
    "taffy.lib.write_taf_index_file(taf_file=alignment_file, index_file=index_file)\n",
    "\n",
    "# Make the Taf Index object\n",
    "taf_index = taffy.lib.TafIndex(index_file, is_maf=False) #False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the following reference sequence intervals: [('hg38.chr22', 22000000, 100000)]\n"
     ]
    }
   ],
   "source": [
    "## Get the names of the sequences in the alignment\n",
    "\n",
    "# First make an alignment reader\n",
    "with taffy.lib.AlignmentReader(alignment_file) as ar:\n",
    "    # Now get the intervals\n",
    "    sequence_intervals = list(taffy.lib.get_reference_sequence_intervals(ar))\n",
    "\n",
    "print(f\"Got the following reference sequence intervals: {sequence_intervals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 3.90037202835083 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "with taffy.lib.AlignmentReader(alignment_file, taf_index=taf_index, sequence_intervals=sequence_intervals) as ar:\n",
    "    for block in ar:\n",
    "        pass\n",
    "print(f\"It took {time.time()-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 4.628628969192505 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "with taffy.lib.AlignmentReader(alignment_file, taf_index=taf_index, sequence_intervals=sequence_intervals) as ar:\n",
    "    for column in taffy.lib.get_column_iterator(ar,\n",
    "                        include_sequence_names=False,\n",
    "                        include_non_ref_columns=False,\n",
    "                        include_column_tags=True):\n",
    "        pass\n",
    "print(f\"It took {time.time()-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 12.872314929962158 seconds, alignment depth: 892\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "## Create a DataLoader for the alignment, print the first few entries and establish the alignment depth\n",
    "def get_alignment_iterator(batch_size=10, num_workers=1):\n",
    "    alignment_iterator = DataLoader(taffy.ml.TorchDatasetAlignmentIterator(alignment_file, \n",
    "                                                                            label_conversion_function=taffy.ml.get_phyloP_label,\n",
    "                                                                            taf_index_file=index_file, \n",
    "                                                                            is_maf=False,\n",
    "                                                                            sequence_intervals=sequence_intervals, \n",
    "                                                                            window_length=1,\n",
    "                                                                            step=1,\n",
    "                                                                            include_non_ref_columns=False,\n",
    "                                                                            include_sequence_names=False, \n",
    "                                                                            include_column_tags=True,\n",
    "                                                                            column_one_hot=True)\n",
    "                                    ,\n",
    "                                    batch_size=batch_size, num_workers=num_workers)\n",
    "    return alignment_iterator\n",
    "\n",
    "start = time.time()\n",
    "for (column, labels), i in zip(get_alignment_iterator(num_workers=1), range(100000000)):\n",
    "    alignment_depth = len(column[0])\n",
    "    #print(column.shape)\n",
    "print(f\"It took {time.time()-start} seconds, alignment depth: {alignment_depth}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5352, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Create a v. basic NN model with one hidden layer\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(alignment_depth * 6, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.flatten(X)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.862984  [   10]\n",
      "loss: 0.202136  [ 1010]\n",
      "loss: 1.050466  [ 2010]\n",
      "loss: 0.740470  [ 3010]\n",
      "loss: 1.269063  [ 4010]\n",
      "loss: 0.398663  [ 5010]\n",
      "loss: 0.170974  [ 6010]\n",
      "loss: 0.737382  [ 7010]\n",
      "loss: 0.710514  [ 8010]\n",
      "loss: 1.002550  [ 9010]\n",
      "loss: 0.239721  [10010]\n",
      "loss: 0.119561  [11010]\n",
      "loss: 0.420180  [12010]\n",
      "loss: 0.592982  [13010]\n",
      "loss: 0.140587  [14010]\n",
      "loss: 0.587670  [15010]\n",
      "loss: 0.163824  [16010]\n",
      "loss: 0.205278  [17010]\n",
      "loss: 0.776681  [18010]\n",
      "loss: 0.151647  [19010]\n",
      "loss: 2.235777  [20010]\n",
      "loss: 4.296134  [21010]\n",
      "loss: 21.038654  [22010]\n",
      "loss: 3.943669  [23010]\n",
      "loss: 1.378535  [24010]\n",
      "loss: 1.566987  [25010]\n",
      "loss: 0.585335  [26010]\n",
      "loss: 2.589970  [27010]\n",
      "loss: 1.587205  [28010]\n",
      "loss: 3.012144  [29010]\n",
      "loss: 10.883647  [30010]\n",
      "loss: 42.542637  [31010]\n",
      "loss: 73.277115  [32010]\n",
      "loss: 19.444645  [33010]\n",
      "loss: 79.294411  [34010]\n",
      "loss: 14.656897  [35010]\n",
      "loss: 6.516555  [36010]\n",
      "loss: 2.427401  [37010]\n",
      "loss: 2.682609  [38010]\n",
      "loss: 19.707516  [39010]\n",
      "loss: 0.504729  [40010]\n",
      "loss: 0.230436  [41010]\n",
      "loss: 3.845927  [42010]\n",
      "loss: 0.307923  [43010]\n",
      "loss: 7.456199  [44010]\n",
      "loss: 0.113970  [45010]\n",
      "loss: 63.769691  [46010]\n",
      "loss: 62.790222  [47010]\n",
      "loss: 22.677616  [48010]\n",
      "loss: 3.242855  [49010]\n",
      "loss: 4.485408  [50010]\n",
      "loss: 0.007277  [51010]\n",
      "loss: 0.274320  [52010]\n",
      "loss: 0.040097  [53010]\n",
      "loss: 0.008927  [54010]\n",
      "loss: 0.222797  [55010]\n",
      "loss: 0.219471  [56010]\n",
      "loss: 0.425651  [57010]\n",
      "loss: 0.038572  [58010]\n",
      "loss: 4.947310  [59010]\n",
      "loss: 16.658585  [60010]\n",
      "loss: 11.081000  [61010]\n",
      "loss: 0.823606  [62010]\n",
      "loss: 6.197070  [63010]\n",
      "loss: 3.284227  [64010]\n",
      "loss: 1.850873  [65010]\n",
      "loss: 0.421390  [66010]\n",
      "loss: 12.102007  [67010]\n",
      "loss: 0.987025  [68010]\n",
      "loss: 12.429835  [69010]\n",
      "loss: 18.447199  [70010]\n",
      "loss: 0.103859  [71010]\n",
      "loss: 17.808882  [72010]\n",
      "loss: 35.020187  [73010]\n",
      "loss: 51.855312  [74010]\n",
      "loss: 4.308669  [75010]\n",
      "loss: 0.328597  [76010]\n",
      "loss: 0.515616  [77010]\n",
      "loss: 0.167965  [78010]\n",
      "loss: 0.025149  [79010]\n",
      "loss: 17.033991  [80010]\n",
      "loss: 0.053814  [81010]\n",
      "loss: 0.002809  [82010]\n",
      "loss: 0.004410  [83010]\n",
      "loss: 8.155947  [84010]\n",
      "loss: 65.076027  [85010]\n",
      "loss: 17.114407  [86010]\n",
      "loss: 47.497997  [87010]\n",
      "loss: 1.296633  [88010]\n",
      "loss: 48.441727  [89010]\n",
      "loss: 9.423103  [90010]\n",
      "loss: 0.596473  [91010]\n",
      "loss: 13.870475  [92010]\n",
      "loss: 3.875450  [93010]\n",
      "loss: 82.917801  [94010]\n",
      "loss: 2.236633  [95010]\n",
      "loss: 3.482162  [96010]\n",
      "loss: 0.465323  [97010]\n",
      "loss: 22.639456  [98010]\n",
      "loss: 6.234703  [99010]\n",
      "It took 40.836116313934326 seconds\n"
     ]
    }
   ],
   "source": [
    "## Training functions\n",
    "\n",
    "loss_fn = nn.MSELoss()  # mean square error # nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "batch_size = 10\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    #size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        y = y.float()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}]\")\n",
    "\n",
    "start = time.time()\n",
    "train(get_alignment_iterator(batch_size=batch_size, num_workers=1), model, loss_fn, optimizer)\n",
    "print(f\"It took {time.time()-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Examples: 100, Avg loss: 0.795898 \n",
      "\n",
      "Test Error: \n",
      " Examples: 200, Avg loss: 0.820571 \n",
      "\n",
      "Test Error: \n",
      " Examples: 300, Avg loss: 1.115243 \n",
      "\n",
      "Test Error: \n",
      " Examples: 400, Avg loss: 1.086500 \n",
      "\n",
      "Test Error: \n",
      " Examples: 500, Avg loss: 1.109569 \n",
      "\n",
      "Test Error: \n",
      " Examples: 600, Avg loss: 1.185645 \n",
      "\n",
      "Test Error: \n",
      " Examples: 700, Avg loss: 1.309862 \n",
      "\n",
      "Test Error: \n",
      " Examples: 800, Avg loss: 1.360681 \n",
      "\n",
      "Test Error: \n",
      " Examples: 900, Avg loss: 1.289412 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1000, Avg loss: 1.297040 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1100, Avg loss: 1.293960 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1200, Avg loss: 1.260192 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1300, Avg loss: 1.232817 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1400, Avg loss: 1.231964 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1500, Avg loss: 1.225998 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1600, Avg loss: 1.228817 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1700, Avg loss: 1.181640 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1800, Avg loss: 1.209933 \n",
      "\n",
      "Test Error: \n",
      " Examples: 1900, Avg loss: 1.188165 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2000, Avg loss: 1.327036 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2100, Avg loss: 1.467952 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2200, Avg loss: 2.278160 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2300, Avg loss: 3.161090 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2400, Avg loss: 3.128075 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2500, Avg loss: 3.218226 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2600, Avg loss: 3.108758 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2700, Avg loss: 3.210422 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2800, Avg loss: 3.153279 \n",
      "\n",
      "Test Error: \n",
      " Examples: 2900, Avg loss: 3.158779 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3000, Avg loss: 3.551489 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3100, Avg loss: 4.183840 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3200, Avg loss: 4.548629 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3300, Avg loss: 5.435240 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3400, Avg loss: 6.479344 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3500, Avg loss: 7.481944 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3600, Avg loss: 7.995080 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3700, Avg loss: 8.390726 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3800, Avg loss: 8.331806 \n",
      "\n",
      "Test Error: \n",
      " Examples: 3900, Avg loss: 8.923039 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4000, Avg loss: 9.050045 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4100, Avg loss: 9.641953 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4200, Avg loss: 9.670974 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4300, Avg loss: 9.897462 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4400, Avg loss: 10.194748 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4500, Avg loss: 10.155684 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4600, Avg loss: 10.025681 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4700, Avg loss: 10.455280 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4800, Avg loss: 10.851565 \n",
      "\n",
      "Test Error: \n",
      " Examples: 4900, Avg loss: 10.919626 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5000, Avg loss: 10.980891 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5100, Avg loss: 10.806347 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5200, Avg loss: 10.712084 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5300, Avg loss: 10.514400 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5400, Avg loss: 10.402574 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5500, Avg loss: 10.217854 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5600, Avg loss: 10.039218 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5700, Avg loss: 9.867103 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5800, Avg loss: 9.700955 \n",
      "\n",
      "Test Error: \n",
      " Examples: 5900, Avg loss: 9.762914 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6000, Avg loss: 10.020670 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6100, Avg loss: 10.157500 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6200, Avg loss: 10.231525 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6300, Avg loss: 10.127061 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6400, Avg loss: 10.034024 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6500, Avg loss: 10.088565 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6600, Avg loss: 9.985446 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6700, Avg loss: 9.948083 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6800, Avg loss: 9.898571 \n",
      "\n",
      "Test Error: \n",
      " Examples: 6900, Avg loss: 9.860369 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7000, Avg loss: 9.978666 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7100, Avg loss: 10.153952 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7200, Avg loss: 10.392540 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7300, Avg loss: 10.450197 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7400, Avg loss: 10.542773 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7500, Avg loss: 10.565521 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7600, Avg loss: 10.472513 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7700, Avg loss: 10.447720 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7800, Avg loss: 10.323414 \n",
      "\n",
      "Test Error: \n",
      " Examples: 7900, Avg loss: 10.272956 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8000, Avg loss: 10.536087 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8100, Avg loss: 10.767191 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8200, Avg loss: 10.637640 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8300, Avg loss: 10.511196 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8400, Avg loss: 10.668807 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8500, Avg loss: 10.836033 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8600, Avg loss: 10.931009 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8700, Avg loss: 11.145054 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8800, Avg loss: 11.258738 \n",
      "\n",
      "Test Error: \n",
      " Examples: 8900, Avg loss: 11.392515 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9000, Avg loss: 11.531347 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9100, Avg loss: 11.536249 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9200, Avg loss: 11.530917 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9300, Avg loss: 11.610752 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9400, Avg loss: 11.721386 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9500, Avg loss: 11.708207 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9600, Avg loss: 12.080202 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9700, Avg loss: 12.262438 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9800, Avg loss: 12.275004 \n",
      "\n",
      "Test Error: \n",
      " Examples: 9900, Avg loss: 12.257168 \n",
      "\n",
      "Test Error: \n",
      " Examples: 10000, Avg loss: 12.355413 \n",
      "\n",
      "Test Error: \n",
      " Avg loss: 12.355413 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test functions\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    #size = len(dataloader.dataset)\n",
    "    #num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y = y.float()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            examples += 1\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            if examples % 100 == 0:\n",
    "                print(f\"Test Error: \\n Examples: {examples}, Avg loss: {test_loss/examples:>8f} \\n\")\n",
    "    #test_loss /= num_batches\n",
    "    #correct /= size\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss/examples:>8f} \\n\")\n",
    "\n",
    "test(get_alignment_iterator(batch_size=batch_size, num_workers=1), model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     train(\u001b[43mtrain_dataloader\u001b[49m, model, loss_fn, optimizer)\n\u001b[1;32m      7\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "## Putting it together\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
